{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-21T18:11:19.035765Z",
     "start_time": "2025-12-21T18:11:19.024372Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch                          # Le cœur de PyTorch : tensors, calculs, GPU\n",
    "import torch.nn as nn                 # Contient toutes les couches du réseau (Conv2D, Linear, etc.)\n",
    "import torch.optim as optim           # Optimiseurs pour entraîner le modèle (Adam, SGD, etc.)\n",
    "from torch.utils.data import DataLoader  # Pour gérer le batching et le shuffle des datasets\n",
    "from torch.utils.data import random_split\n",
    "from torchvision import datasets, transforms, models  # Outils pour datasets, transformations et modèles pré-entraînés\n",
    "from torch import tensor\n",
    "\n",
    "import numpy as np                    # Manipulation de tableaux, conversion images → tensors\n",
    "import pandas as pd                   # Pour lire et écrire le fichier test_template.csv\n",
    "import os                             # Gestion des fichiers et dossiers\n",
    "\n",
    "# -----------------------------\n",
    "# Librairies pour la visualisation\n",
    "# -----------------------------\n",
    "import matplotlib.pyplot as plt       # Visualiser les images et tracer des courbes\n",
    "\n",
    "# -----------------------------\n",
    "# Librairies pour la vision par ordinateur\n",
    "# -----------------------------\n",
    "import cv2                            # Capturer la webcam et détecter les visages\n",
    "from PIL import Image \n",
    "from skimage import io\n",
    "import matplotlib.pyplot as plt\n",
    "import os, random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf8e8b82b9cec0fe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-21T18:11:19.415221Z",
     "start_time": "2025-12-21T18:11:19.072187Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'neutral')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "emotions = ['neutral', 'angry', 'fear', 'happy', 'sad', 'surprise', 'disgust']\n",
    "emotion = 'neutral'\n",
    "x = random.randint(0, 6)\n",
    "path = f'data/dataTrain/train/{emotion}/'\n",
    "img = io.imread(os.path.join(path, random.choice(os.listdir(path))))\n",
    "plt.imshow(img, cmap='gray')\n",
    "plt.title(emotion)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d0c74cba097aa9d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-21T18:11:19.532552Z",
     "start_time": "2025-12-21T18:11:19.433898Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "happy : 7215\n",
      "sad : 4830\n",
      "fear : 4097\n",
      "surprise : 3171\n",
      "neutral : 4965\n",
      "angry : 3995\n",
      "disgust : 436\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "for emotion in os.listdir('data/dataTrain/train'):\n",
    "    print(emotion, \":\", len(os.listdir(os.path.join('data/dataTrain/train', emotion))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc8ee914af4626b4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-21T18:11:19.760378Z",
     "start_time": "2025-12-21T18:11:19.653588Z"
    }
   },
   "outputs": [],
   "source": [
    "#Éviter le changement dans validation (on veut avoir les images réels)\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=3),\n",
    "    \n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "    transforms.RandomRotation(10),        # rotation ±10°\n",
    "    transforms.RandomResizedCrop(224, scale=(0.9, 1.0)),  # zoom léger\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.485, 0.456, 0.406),\n",
    "                         std=(0.229, 0.224, 0.225))\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=3),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=(0.485, 0.456, 0.406),\n",
    "        std=(0.229, 0.224, 0.225)\n",
    "    )\n",
    "])\n",
    "\n",
    "train_data = datasets.ImageFolder(\"data/dataTrain/train\", transform=train_transform)\n",
    "val_data   = datasets.ImageFolder(\"data/dataTrain/train\", transform=val_transform)\n",
    "test_data  = datasets.ImageFolder(\"data/dataTest\", transform=val_transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f799ac6eb5969f1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-21T18:11:19.856250Z",
     "start_time": "2025-12-21T18:11:19.793274Z"
    }
   },
   "outputs": [],
   "source": [
    "# Split train / validation\n",
    "train_size = int(0.8 * len(train_data))\n",
    "validation_size = len(train_data) - train_size\n",
    "train_dataset, validation_dataset = random_split(train_data, [train_size, validation_size])\n",
    "\n",
    "# ⚠️ Remplacer le dataset de validation par celui sans augmentation\n",
    "validation_dataset.dataset = val_data\n",
    "\n",
    "# Créer les DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True) # 128 trop aggressif au debut \n",
    "validation_loader = DataLoader(validation_dataset, batch_size=64, shuffle=False)\n",
    "test_loader  = DataLoader(test_data, batch_size=64, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14485ec",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1aab4e647575f7bb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-21T18:11:20.332715Z",
     "start_time": "2025-12-21T18:11:19.875120Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Sequential(\n",
      "    (0): Dropout(p=0.5, inplace=False)\n",
      "    (1): Linear(in_features=512, out_features=7, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/massylia/Desktop/HackatonCodeML-main/.venv/lib/python3.13/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/massylia/Desktop/HackatonCodeML-main/.venv/lib/python3.13/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# --- Modèle : ResNet18 fine-tuning ---\n",
    "modeleEmotions = models.resnet18(pretrained=True)\n",
    "\n",
    "#degele aussi layer 3 (avant que 4) \n",
    "for name, param in modeleEmotions.named_parameters():\n",
    "    if \"layer3\" in name or \"layer4\" in name or \"fc\" in name:\n",
    "        param.requires_grad = True\n",
    "    else:\n",
    "        param.requires_grad = False\n",
    "\n",
    "nbEmotions = 7\n",
    "modeleEmotions.fc = nn.Sequential(\n",
    "    nn.Dropout(p=0.5),\n",
    "    nn.Linear(512, nbEmotions)\n",
    ")\n",
    "\n",
    "\n",
    "print(modeleEmotions) # Modèle de classification des émotions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154f4b8c91daf5c5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-21T18:25:20.794832Z",
     "start_time": "2025-12-21T18:25:20.701385Z"
    }
   },
   "outputs": [],
   "source": [
    "#loss function on ne veut pas geler toutes les couches. Sinon, le modele s'ameliore trop lentement\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "modeleEmotions = modeleEmotions.to(device)\n",
    "\n",
    "# --- Optimiseur:recopies / recoles la ligne de l’optimizer juste après avoir changé requires_grad.\n",
    "# (pour que l’optimizeur reconnaisse les nouvelles couches )---\n",
    "optimizer = torch.optim.Adam(\n",
    "    filter(lambda p: p.requires_grad, modeleEmotions.parameters()),\n",
    "    lr=1e-4\n",
    ")\n",
    "\n",
    "# Scheduler qui réduit le learning rate. eviter stagnation \n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    mode='min',          # surveille la val_loss\n",
    "    factor=0.5,          # réduit LR de moitié\n",
    "    patience=1,          # attend 1 epoch avant de réduire\n",
    ")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6b327164ef180ec3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-21T18:25:27.575737Z",
     "start_time": "2025-12-21T18:25:22.846111Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs shape: torch.Size([64, 7])\n",
      "labels shape: torch.Size([64])\n",
      "labels dtype: torch.int64\n",
      "labels min/max: 0 6\n"
     ]
    }
   ],
   "source": [
    "#sanity check\n",
    "images, labels = next(iter(train_loader))\n",
    "images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "outputs = modeleEmotions(images)\n",
    "print(\"outputs shape:\", outputs.shape)   # doit être (64, 7) ou (dernier batch, 7)\n",
    "print(\"labels shape:\", labels.shape)     # doit être (64,) ou (dernier batch,)\n",
    "print(\"labels dtype:\", labels.dtype)     # doit être torch.int64 (Long)\n",
    "print(\"labels min/max:\", labels.min().item(), labels.max().item())  # doit être 0..6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529154f831b8c8a8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-22T21:44:18.751042Z",
     "start_time": "2025-12-21T19:00:15.533475Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 | Train loss 1.3441 acc 0.4970 | Val loss 1.0847 acc 0.5787\n",
      "Epoch 2/10 | Train loss 1.0664 acc 0.6001 | Val loss 1.0533 acc 0.6082\n",
      "Epoch 3/10 | Train loss 0.9683 acc 0.6404 | Val loss 0.9778 acc 0.6217\n",
      "Epoch 4/10 | Train loss 0.8954 acc 0.6682 | Val loss 0.9696 acc 0.6407\n",
      "Epoch 5/10 | Train loss 0.8208 acc 0.6938 | Val loss 0.9712 acc 0.6350\n",
      "Epoch 6/10 | Train loss 0.7614 acc 0.7184 | Val loss 0.9605 acc 0.6526\n",
      "Epoch 7/10 | Train loss 0.6973 acc 0.7436 | Val loss 0.9729 acc 0.6543\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 38\u001b[39m\n\u001b[32m     36\u001b[39m \u001b[38;5;66;03m# --- Boucle principale ---\u001b[39;00m\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m     train_loss, train_acc = \u001b[43mrun_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodeleEmotions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     39\u001b[39m     val_loss, val_acc = run_one_epoch(modeleEmotions, validation_loader, training=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m     41\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m | \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     42\u001b[39m           \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTrain loss \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m acc \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m | \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     43\u001b[39m           \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mVal loss \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m acc \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 14\u001b[39m, in \u001b[36mrun_one_epoch\u001b[39m\u001b[34m(model, loader, training)\u001b[39m\n\u001b[32m     11\u001b[39m total = \u001b[32m0\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.set_grad_enabled(training):\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mloader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m        \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/HackatonCodeML-main/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:732\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    729\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    730\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    731\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m732\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    733\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    734\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    735\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    736\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    738\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/HackatonCodeML-main/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:788\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    786\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    787\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m788\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    789\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    790\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/HackatonCodeML-main/.venv/lib/python3.13/site-packages/torch/utils/data/_utils/fetch.py:50\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.auto_collation:\n\u001b[32m     49\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.dataset, \u001b[33m\"\u001b[39m\u001b[33m__getitems__\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.dataset.__getitems__:\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m         data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     52\u001b[39m         data = [\u001b[38;5;28mself\u001b[39m.dataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/HackatonCodeML-main/.venv/lib/python3.13/site-packages/torch/utils/data/dataset.py:416\u001b[39m, in \u001b[36mSubset.__getitems__\u001b[39m\u001b[34m(self, indices)\u001b[39m\n\u001b[32m    414\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.dataset.__getitems__([\u001b[38;5;28mself\u001b[39m.indices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m    415\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m416\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/HackatonCodeML-main/.venv/lib/python3.13/site-packages/torchvision/datasets/folder.py:245\u001b[39m, in \u001b[36mDatasetFolder.__getitem__\u001b[39m\u001b[34m(self, index)\u001b[39m\n\u001b[32m    237\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    238\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m    239\u001b[39m \u001b[33;03m    index (int): Index\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    242\u001b[39m \u001b[33;03m    tuple: (sample, target) where target is class_index of the target class.\u001b[39;00m\n\u001b[32m    243\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    244\u001b[39m path, target = \u001b[38;5;28mself\u001b[39m.samples[index]\n\u001b[32m--> \u001b[39m\u001b[32m245\u001b[39m sample = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mloader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    246\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    247\u001b[39m     sample = \u001b[38;5;28mself\u001b[39m.transform(sample)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/HackatonCodeML-main/.venv/lib/python3.13/site-packages/torchvision/datasets/folder.py:284\u001b[39m, in \u001b[36mdefault_loader\u001b[39m\u001b[34m(path)\u001b[39m\n\u001b[32m    282\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m accimage_loader(path)\n\u001b[32m    283\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m284\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpil_loader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/HackatonCodeML-main/.venv/lib/python3.13/site-packages/torchvision/datasets/folder.py:262\u001b[39m, in \u001b[36mpil_loader\u001b[39m\u001b[34m(path)\u001b[39m\n\u001b[32m    260\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpil_loader\u001b[39m(path: Union[\u001b[38;5;28mstr\u001b[39m, Path]) -> Image.Image:\n\u001b[32m    261\u001b[39m     \u001b[38;5;66;03m# open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m262\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m    263\u001b[39m         img = Image.open(f)\n\u001b[32m    264\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m img.convert(\u001b[33m\"\u001b[39m\u001b[33mRGB\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "\n",
    "def run_one_epoch(model, loader, training: bool):\n",
    "    if training:\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.set_grad_enabled(training):\n",
    "        for images, labels in loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            if training:\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(images)              # (batch, nbEmotions)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            if training:\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            total_loss += loss.item() * images.size(0)\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    return total_loss / total, correct / total\n",
    "\n",
    "\n",
    "# --- Boucle principale ---\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_acc = run_one_epoch(modeleEmotions, train_loader, training=True)\n",
    "    val_loss, val_acc = run_one_epoch(modeleEmotions, validation_loader, training=False)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} | \"\n",
    "          f\"Train loss {train_loss:.4f} acc {train_acc:.4f} | \"\n",
    "          f\"Val loss {val_loss:.4f} acc {val_acc:.4f}\")\n",
    "\n",
    "scheduler.step(val_loss) #éviter stagnation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
