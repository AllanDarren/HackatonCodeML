{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T03:28:40.746026Z",
     "start_time": "2025-12-27T03:28:40.644326Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch                          # Le cœur de PyTorch : tensors, calculs, GPU\n",
    "import torch.nn as nn                 # Contient toutes les couches du réseau (Conv2D, Linear, etc.)\n",
    "import torch.optim as optim           # Optimiseurs pour entraîner le modèle (Adam, SGD, etc.)\n",
    "from torch.utils.data import DataLoader  # Pour gérer le batching et le shuffle des datasets\n",
    "from torch.utils.data import random_split\n",
    "from torchvision import datasets, transforms, models  # Outils pour datasets, transformations et modèles pré-entraînés\n",
    "\n",
    "\n",
    "import numpy as np                    # Manipulation de tableaux, conversion images → tensors\n",
    "import pandas as pd                   # Pour lire et écrire le fichier test_template.csv\n",
    "import os                             # Gestion des fichiers et dossiers\n",
    "\n",
    "# -----------------------------\n",
    "# Librairies pour la visualisation\n",
    "# -----------------------------\n",
    "import matplotlib.pyplot as plt       # Visualiser les images et tracer des courbes\n",
    "\n",
    "# -----------------------------\n",
    "# Librairies pour la vision par ordinateur\n",
    "# -----------------------------\n",
    "import cv2                            # Capturer la webcam et détecter les visages\n",
    "from PIL import Image \n",
    "from skimage import io\n",
    "import matplotlib.pyplot as plt\n",
    "import os, random"
   ],
   "id": "aeb6b9ecbc34dcd5",
   "outputs": [],
   "execution_count": 92
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T03:28:40.988806Z",
     "start_time": "2025-12-27T03:28:40.783490Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#observations des images\n",
    "emotions = ['neutral', 'angry', 'fear', 'happy', 'sad', 'surprise', 'disgust']\n",
    "emotion = 'neutral'\n",
    "x = random.randint(0, 6)\n",
    "path = f'data/dataTrain/train/{emotions[x]}/'\n",
    "img = io.imread(os.path.join(path, random.choice(os.listdir(path))))\n",
    "plt.imshow(img, cmap='gray')\n",
    "plt.title(emotions[x])\n",
    "plt.show()"
   ],
   "id": "7514b82e2992d93f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 93
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T03:28:47.171532Z",
     "start_time": "2025-12-27T03:28:47.061283Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "for emotion in os.listdir('data/dataTrain/train'):\n",
    "    print(emotion, \":\", len(os.listdir(os.path.join('data/dataTrain/train', emotion))))\n"
   ],
   "id": "e36a18a5ab11cb4a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "happy : 7215\n",
      "sad : 4830\n",
      "fear : 4097\n",
      "surprise : 3171\n",
      "neutral : 4965\n",
      "angry : 3995\n",
      "disgust : 436\n"
     ]
    }
   ],
   "execution_count": 94
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T03:28:48.485324Z",
     "start_time": "2025-12-27T03:28:48.074107Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- Modèle : ResNet18 fine-tuning ---\n",
    "modeleEmotions = models.resnet18(pretrained=True)\n",
    "\n",
    "# Dé-geler uniquement le dernier bloc convolutionnel + fc\n",
    "for name, param in modeleEmotions.named_parameters():\n",
    "    if \"layer4\" in name or \"fc\" in name:\n",
    "        param.requires_grad = True\n",
    "    else:\n",
    "        param.requires_grad = False\n",
    "\n",
    "nbEmotions = 7\n",
    "modeleEmotions.fc = nn.Sequential(\n",
    "    nn.Dropout(p=0.5),\n",
    "    nn.Linear(512, nbEmotions)\n",
    ")\n",
    "\n",
    "\n",
    "print(modeleEmotions) # Modèle de classification des émotions"
   ],
   "id": "aae1f6eb45864f23",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/allankamta/PycharmProjects/pythonProject/venv/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/allankamta/PycharmProjects/pythonProject/venv/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Sequential(\n",
      "    (0): Dropout(p=0.5, inplace=False)\n",
      "    (1): Linear(in_features=512, out_features=7, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "execution_count": 95
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T03:28:54.834765Z",
     "start_time": "2025-12-27T03:28:49.373104Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#loss function on ne veut pas geler toutes les couches. Sinon, le modele s'ameliore trop lentement\n",
    "def get_device():\n",
    "    # NVIDIA GPU (Windows/Linux, parfois Mac via eGPU mais rare)\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    # Apple Silicon (Mac M1/M2/M3)\n",
    "    if hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "        return torch.device(\"mps\")\n",
    "    # Fallback\n",
    "    return torch.device(\"cpu\")\n",
    "print(torch.__version__)\n",
    "device = get_device()\n",
    "print(\"Using device:\", device)\n",
    "modeleEmotions = modeleEmotions.to(device)\n"
   ],
   "id": "1115bd093b83729d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9.0\n",
      "Using device: mps\n"
     ]
    }
   ],
   "execution_count": 96
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T03:28:56.143184Z",
     "start_time": "2025-12-27T03:28:56.125265Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels = 3), # images en niveaux de gris rgb\n",
    "    transforms.Resize((224, 224)),\n",
    "\n",
    "# data augmentation. eviter apprendre par coeur (chaque epoch change les images pour paraitre différement pour le modele)\n",
    "    transforms.RandomHorizontalFlip(p = 0.5),           # augmentation horizontale\n",
    "    transforms.ColorJitter(brightness=0.8, contrast=0.8),\n",
    "\n",
    "    transforms.ToTensor(),             # convertit en tenseur [0,1]\n",
    "    transforms.Normalize(mean=(0.485, 0.456, 0.406),\n",
    "                         std=(0.229, 0.224, 0.225)) # normalise entre des range que imagenet comprend bien\n",
    "])\n",
    "eval_transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels = 3),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.485, 0.456, 0.406),\n",
    "                         std=(0.229, 0.224, 0.225))\n",
    "])\n"
   ],
   "id": "37ff90f6fdca3e4e",
   "outputs": [],
   "execution_count": 97
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T03:28:57.132750Z",
     "start_time": "2025-12-27T03:28:56.909016Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#80% des données d'entrainement pour l'entrainement et 20% pour validation\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "train_root = \"data/dataTrain/train\"\n",
    "\n",
    "# dataset \"base\" juste pour récupérer les labels (targets)\n",
    "base_ds = datasets.ImageFolder(train_root, transform=None)\n",
    "y = base_ds.targets  # liste d'entiers 0..6\n",
    "\n",
    "# séparation équilibré entre train et val\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "train_idx, val_idx = next(sss.split(X=[0]*len(y), y=y))\n",
    "\n",
    "# datasets avec les bons transforms\n",
    "train_full = datasets.ImageFolder(train_root, transform=train_transform)\n",
    "val_full   = datasets.ImageFolder(train_root, transform=eval_transform)\n",
    "\n",
    "train_dataset = Subset(train_full, train_idx)\n",
    "val_dataset   = Subset(val_full, val_idx)\n",
    "\n",
    "print(\"Train size:\", len(train_dataset), \"Val size:\", len(val_dataset))"
   ],
   "id": "e4e50d7a759bb228",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 22967 Val size: 5742\n"
     ]
    }
   ],
   "execution_count": 98
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T03:37:32.329094Z",
     "start_time": "2025-12-27T03:37:32.264957Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "\n",
    "# compter les classes dans le train split\n",
    "train_targets = torch.tensor([y[i] for i in train_idx], dtype = torch.long)  # labels du train\n",
    "\n",
    "#compte le nombre d'images par class\n",
    "class_counts = torch.bincount(train_targets)\n",
    "class_weights = 1.0 / class_counts.float() #attribut une proba a chaque class\n",
    "class_weights = class_weights / class_weights.mean()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights.to(device))\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, num_workers=2, pin_memory=True)\n",
    "validation_loader = DataLoader(val_dataset, batch_size=128, shuffle=False, num_workers=2, pin_memory=True)\n",
    "# test_loader  = DataLoader(test_data, batch_size=128, shuffle=False)"
   ],
   "id": "13a67a1fc4cf2655",
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torch' has no attribute 'MPSFloatType'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[105], line 5\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdata\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m DataLoader, WeightedRandomSampler\n\u001B[1;32m      4\u001B[0m \u001B[38;5;66;03m# compter les classes dans le train split\u001B[39;00m\n\u001B[0;32m----> 5\u001B[0m train_targets \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mtensor([y[i] \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m train_idx], dtype \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mMPSFloatType\u001B[49m)  \u001B[38;5;66;03m# labels du train\u001B[39;00m\n\u001B[1;32m      7\u001B[0m \u001B[38;5;66;03m#compte le nombre d'images par class\u001B[39;00m\n\u001B[1;32m      8\u001B[0m class_counts \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mbincount(train_targets)\n",
      "File \u001B[0;32m~/PycharmProjects/pythonProject/venv/lib/python3.11/site-packages/torch/__init__.py:2757\u001B[0m, in \u001B[0;36m__getattr__\u001B[0;34m(name)\u001B[0m\n\u001B[1;32m   2754\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;129;01min\u001B[39;00m _lazy_modules:\n\u001B[1;32m   2755\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m importlib\u001B[38;5;241m.\u001B[39mimport_module(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mname\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;18m__name__\u001B[39m)\n\u001B[0;32m-> 2757\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAttributeError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodule \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m has no attribute \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mname\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[0;31mAttributeError\u001B[0m: module 'torch' has no attribute 'MPSFloatType'"
     ]
    }
   ],
   "execution_count": 105
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T03:35:01.845289Z",
     "start_time": "2025-12-27T03:35:01.584749Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- Modèle : ResNet18 fine-tuning ---\n",
    "modeleEmotions = models.resnet18(pretrained=True)\n",
    "\n",
    "# Dé-geler uniquement le dernier bloc convolutionnel + fc\n",
    "for name, param in modeleEmotions.named_parameters():\n",
    "    if \"layer4\" in name or \"fc\" in name:\n",
    "        param.requires_grad = True\n",
    "    else:\n",
    "        param.requires_grad = False\n",
    "\n",
    "nbEmotions = 7\n",
    "modeleEmotions.fc = nn.Sequential(\n",
    "    nn.Dropout(p=0.5),\n",
    "    nn.Linear(512, nbEmotions)\n",
    ")\n",
    "\n",
    "\n",
    "print(modeleEmotions) # Modèle de classification des émotions\n"
   ],
   "id": "805228ee506d7176",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/allankamta/PycharmProjects/pythonProject/venv/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/allankamta/PycharmProjects/pythonProject/venv/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Sequential(\n",
      "    (0): Dropout(p=0.5, inplace=False)\n",
      "    (1): Linear(in_features=512, out_features=7, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "execution_count": 102
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T03:35:04.453444Z",
     "start_time": "2025-12-27T03:35:04.443609Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- Optimiseur et scheduler ---\n",
    "optimizer = torch.optim.Adam(\n",
    "    filter(lambda p: p.requires_grad, modeleEmotions.parameters()),\n",
    "    lr=1e-4\n",
    ")\n",
    "\n",
    "# Scheduler qui réduit le learning rate. eviter stagnation\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    mode='min',          # surveille la val_loss\n",
    "    factor=0.5,          # réduit LR de moitié\n",
    "    patience=1,          # attend 1 epoch avant de réduire\n",
    ")"
   ],
   "id": "2a63d1f1a27beeae",
   "outputs": [],
   "execution_count": 103
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T03:39:38.310052Z",
     "start_time": "2025-12-27T03:39:10.665329Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#sanity check\n",
    "images, labels = next(iter(train_loader))\n",
    "images, labels = images.to(device), labels.to(device)\n",
    "modeleEmotions.to(device)\n",
    "\n",
    "outputs = modeleEmotions(images)\n",
    "print(\"outputs shape:\", outputs.shape)\n",
    "print(\"labels shape:\", labels.shape)\n",
    "print(\"labels dtype:\", labels.dtype)\n",
    "print(\"labels min/max:\", labels.min().item(), labels.max().item())  # doit être 0..6"
   ],
   "id": "4fd40c5040dab02b",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/allankamta/PycharmProjects/pythonProject/venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs shape: torch.Size([128, 7])\n",
      "labels shape: torch.Size([128])\n",
      "labels dtype: torch.int64\n",
      "labels min/max: 0 6\n"
     ]
    }
   ],
   "execution_count": 106
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T03:50:53.072721Z",
     "start_time": "2025-12-27T03:44:59.425087Z"
    }
   },
   "cell_type": "code",
   "source": [
    "num_epochs = 20 # 20 epoch de base pour chaque modification\n",
    "\n",
    "#fonction principace d'entraiment et de validation\n",
    "def run_one_epoch(model, loader, training: bool):\n",
    "    if training:\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.set_grad_enabled(training):\n",
    "        for images, labels in loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            if training:\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(images)              # (batch, nbEmotions)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            if training:\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            total_loss += loss.item() * images.size(0)\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    return total_loss / total, correct / total\n",
    "\n",
    "\n",
    "# --- Boucle principale ---\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_acc = run_one_epoch(modeleEmotions, train_loader, training=True)\n",
    "    val_loss, val_acc = run_one_epoch(modeleEmotions, validation_loader, training=False)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} | \"\n",
    "          f\"Train loss {train_loss:.4f} acc {train_acc:.4f} | \"\n",
    "          f\"Val loss {val_loss:.4f} acc {val_acc:.4f}\")\n",
    "\n"
   ],
   "id": "a10c848ec8acb4c6",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/allankamta/PycharmProjects/pythonProject/venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 | Train loss 1.7459 acc 0.3266 | Val loss 1.3444 acc 0.4904\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[107], line 39\u001B[0m\n\u001B[1;32m     37\u001B[0m \u001B[38;5;66;03m# --- Boucle principale ---\u001B[39;00m\n\u001B[1;32m     38\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(num_epochs):\n\u001B[0;32m---> 39\u001B[0m     train_loss, train_acc \u001B[38;5;241m=\u001B[39m \u001B[43mrun_one_epoch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodeleEmotions\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtraining\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[1;32m     40\u001B[0m     val_loss, val_acc \u001B[38;5;241m=\u001B[39m run_one_epoch(modeleEmotions, validation_loader, training\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[1;32m     42\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEpoch \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mepoch\u001B[38;5;241m+\u001B[39m\u001B[38;5;241m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mnum_epochs\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m | \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     43\u001B[0m           \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTrain loss \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtrain_loss\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.4f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m acc \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtrain_acc\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.4f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m | \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     44\u001B[0m           \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mVal loss \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mval_loss\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.4f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m acc \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mval_acc\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.4f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "Cell \u001B[0;32mIn[107], line 15\u001B[0m, in \u001B[0;36mrun_one_epoch\u001B[0;34m(model, loader, training)\u001B[0m\n\u001B[1;32m     12\u001B[0m total \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m     14\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mset_grad_enabled(training):\n\u001B[0;32m---> 15\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mimages\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlabels\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mloader\u001B[49m\u001B[43m:\u001B[49m\n\u001B[1;32m     16\u001B[0m \u001B[43m        \u001B[49m\u001B[43mimages\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mimages\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     17\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlabels\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mlabels\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/pythonProject/venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:732\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    729\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    730\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[1;32m    731\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[0;32m--> 732\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    733\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m    734\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[1;32m    735\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable\n\u001B[1;32m    736\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    737\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called\n\u001B[1;32m    738\u001B[0m ):\n",
      "File \u001B[0;32m~/PycharmProjects/pythonProject/venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1470\u001B[0m, in \u001B[0;36m_MultiProcessingDataLoaderIter._next_data\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1467\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1468\u001B[0m     \u001B[38;5;66;03m# no valid `self._rcvd_idx` is found (i.e., didn't break)\u001B[39;00m\n\u001B[1;32m   1469\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_persistent_workers:\n\u001B[0;32m-> 1470\u001B[0m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_shutdown_workers\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1471\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mStopIteration\u001B[39;00m\n\u001B[1;32m   1473\u001B[0m \u001B[38;5;66;03m# Now `self._rcvd_idx` is the batch index we want to fetch\u001B[39;00m\n\u001B[1;32m   1474\u001B[0m \n\u001B[1;32m   1475\u001B[0m \u001B[38;5;66;03m# Check if the next sample has already been generated\u001B[39;00m\n",
      "File \u001B[0;32m~/PycharmProjects/pythonProject/venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1618\u001B[0m, in \u001B[0;36m_MultiProcessingDataLoaderIter._shutdown_workers\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1613\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_mark_worker_as_unavailable(worker_id, shutdown\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m   1614\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m w \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_workers:\n\u001B[1;32m   1615\u001B[0m     \u001B[38;5;66;03m# We should be able to join here, but in case anything went\u001B[39;00m\n\u001B[1;32m   1616\u001B[0m     \u001B[38;5;66;03m# wrong, we set a timeout and if the workers fail to join,\u001B[39;00m\n\u001B[1;32m   1617\u001B[0m     \u001B[38;5;66;03m# they are killed in the `finally` block.\u001B[39;00m\n\u001B[0;32m-> 1618\u001B[0m     \u001B[43mw\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mjoin\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m_utils\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mMP_STATUS_CHECK_INTERVAL\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1619\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m q \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_index_queues:\n\u001B[1;32m   1620\u001B[0m     q\u001B[38;5;241m.\u001B[39mcancel_join_thread()\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/process.py:149\u001B[0m, in \u001B[0;36mBaseProcess.join\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    147\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_parent_pid \u001B[38;5;241m==\u001B[39m os\u001B[38;5;241m.\u001B[39mgetpid(), \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcan only join a child process\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m    148\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_popen \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcan only join a started process\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m--> 149\u001B[0m res \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_popen\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwait\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    150\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m res \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    151\u001B[0m     _children\u001B[38;5;241m.\u001B[39mdiscard(\u001B[38;5;28mself\u001B[39m)\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/popen_fork.py:40\u001B[0m, in \u001B[0;36mPopen.wait\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m     38\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m timeout \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m     39\u001B[0m     \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mmultiprocessing\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mconnection\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m wait\n\u001B[0;32m---> 40\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[43mwait\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msentinel\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m:\n\u001B[1;32m     41\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m     42\u001B[0m \u001B[38;5;66;03m# This shouldn't block if wait() returned successfully.\u001B[39;00m\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/connection.py:930\u001B[0m, in \u001B[0;36mwait\u001B[0;34m(object_list, timeout)\u001B[0m\n\u001B[1;32m    927\u001B[0m     deadline \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mmonotonic() \u001B[38;5;241m+\u001B[39m timeout\n\u001B[1;32m    929\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[0;32m--> 930\u001B[0m     ready \u001B[38;5;241m=\u001B[39m \u001B[43mselector\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mselect\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    931\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m ready:\n\u001B[1;32m    932\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m [key\u001B[38;5;241m.\u001B[39mfileobj \u001B[38;5;28;01mfor\u001B[39;00m (key, events) \u001B[38;5;129;01min\u001B[39;00m ready]\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/selectors.py:415\u001B[0m, in \u001B[0;36m_PollLikeSelector.select\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    413\u001B[0m ready \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m    414\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 415\u001B[0m     fd_event_list \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_selector\u001B[38;5;241m.\u001B[39mpoll(timeout)\n\u001B[1;32m    416\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mInterruptedError\u001B[39;00m:\n\u001B[1;32m    417\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m ready\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 107
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "5bc47b6dd91a78e4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "a44efeaa17ec9190"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
